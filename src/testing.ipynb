{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dashboard import DEX_LIST, DEX_LIQUIDITY_METRIC_MAP\n",
    "from constants import *\n",
    "\n",
    "# open all files in test_results, create a dict for each and a combined dict pool_dict\n",
    "'''DEX_pool_dict = {}\n",
    "for dex in DEX_LIST:\n",
    "    with open(f'test_results/pool_dict_{dex}.json', 'r') as f:\n",
    "        DEX_pool_dict[dex] = json.load(f)\n",
    "\n",
    "# create a combined dict by merging all dicts in pool_dict\n",
    "combined_dict = {}\n",
    "for dex in DEX_LIST:\n",
    "    for key in DEX_pool_dict[dex]:\n",
    "        combined_dict[key] = DEX_pool_dict[dex][key]'''\n",
    "        \n",
    "# open pool_dict.json from test_results\n",
    "with open('data/pool_dict.json', 'r') as f:\n",
    "    pool_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "\n",
    "MIN_LIQUIDITY_THRESHOLD = 1_000_000\n",
    "MAX_LIQUIDITY_THRESHOLD = 200_000_000_000\n",
    "\n",
    "# Initialize empty dictionary for storing token liquidity\n",
    "token_liquidity = {}\n",
    "\n",
    "def refresh_matrix(pool_dict):\n",
    "    # Calculate total liquidity for each token\n",
    "    for pool in pool_dict.values():\n",
    "        token0_id = pool['token0']['id']\n",
    "        token1_id = pool['token1']['id']\n",
    "        \n",
    "        # Ignore tokens with punctuation in their symbol, band-aid fix for synthetic and irreputable tokens\n",
    "        if any(char in string.punctuation for char in pool['token0']['symbol']):\n",
    "            continue\n",
    "        if any(char in string.punctuation for char in pool['token1']['symbol']):\n",
    "            continue\n",
    "        # Ignore Balancer_V1 and Balancer_V2 pools, another band-aid fix as their USD price is not accurate\n",
    "        if pool['protocol'] == BALANCER_V1 or pool['protocol'] == BALANCER_V2 or pool['protocol'] == DODO:\n",
    "            continue\n",
    "\n",
    "        liquidity = float(pool[DEX_LIQUIDITY_METRIC_MAP[pool['protocol']]])\n",
    "        reserve0 = float(pool['reserve0'])\n",
    "        reserve1 = float(pool['reserve1'])\n",
    "        total_reserve = reserve0 + reserve1\n",
    "\n",
    "        # Calculate and add liquidity to respective tokens\n",
    "        if token0_id in token_liquidity:\n",
    "            try:\n",
    "                token_liquidity[token0_id]['liquidity'] += liquidity * (reserve0 / total_reserve)\n",
    "            except ZeroDivisionError:\n",
    "                token_liquidity[token0_id]['liquidity'] += 0\n",
    "        else:\n",
    "            try:\n",
    "                token_liquidity[token0_id] = {'symbol': pool['token0']['symbol'], 'liquidity': liquidity * (reserve0 / total_reserve)}\n",
    "            except ZeroDivisionError:\n",
    "                token_liquidity[token0_id] = {'symbol': pool['token0']['symbol'], 'liquidity': 0}\n",
    "        if token1_id in token_liquidity:\n",
    "            try:\n",
    "                token_liquidity[token1_id]['liquidity'] += liquidity * (reserve1 / total_reserve)\n",
    "            except ZeroDivisionError:\n",
    "                token_liquidity[token1_id]['liquidity'] += 0\n",
    "        else:\n",
    "            try:\n",
    "                token_liquidity[token1_id] = {'symbol': pool['token1']['symbol'], 'liquidity': liquidity * (reserve1 / total_reserve)}\n",
    "            except ZeroDivisionError:\n",
    "                token_liquidity[token1_id] = {'symbol': pool['token1']['symbol'], 'liquidity': 0}\n",
    "\n",
    "    # sort token_liquidity by liquidity\n",
    "    sorted_token_liquidity = dict(sorted(token_liquidity.items(), key=lambda item: item[1]['liquidity'], reverse=True))\n",
    "\n",
    "    # drop any tokens with liquidity less than LIQUIDITY_THRESHOLD\n",
    "    trimmed_sorted_token_liquidity = {}\n",
    "\n",
    "    # Iterate over sorted_token_liquidity dictionary\n",
    "    for k, v in sorted_token_liquidity.items():\n",
    "        # If the liquidity is greater than the threshold, add it to the new dictionary\n",
    "        if v['liquidity'] > MIN_LIQUIDITY_THRESHOLD and v['liquidity'] < MAX_LIQUIDITY_THRESHOLD:\n",
    "            trimmed_sorted_token_liquidity[k] = v\n",
    "\n",
    "    # Initialize a new dictionary for the trimmed pool_dict\n",
    "    trimmed_pool_dict = {}\n",
    "\n",
    "    # Iterate over the original pool_dict\n",
    "    for pool_id, pool in pool_dict.items():\n",
    "        # Get the IDs of the tokens in the current pool\n",
    "        token0_id = pool['token0']['id']\n",
    "        token1_id = pool['token1']['id']\n",
    "\n",
    "        # If either of the tokens is in trimmed_sorted_token_liquidity, add the pool to trimmed_pool_dict\n",
    "        if token0_id in trimmed_sorted_token_liquidity or token1_id in trimmed_sorted_token_liquidity:\n",
    "            trimmed_pool_dict[pool_id] = pool\n",
    "\n",
    "    trimmed_sorted_pools = sorted(trimmed_pool_dict.values(), key=lambda pool: float(pool[DEX_LIQUIDITY_METRIC_MAP[pool['protocol']]]), reverse=True)\n",
    "\n",
    "    # Drop extreme pools, another band-aid fix\n",
    "    # trimmed_sorted_pool_dict = {k: v for k, v in trimmed_sorted_pools.items() if float(v[DEX_LIQUIDITY_METRIC_MAP[v['protocol']]]) > MIN_LIQUIDITY_THRESHOLD and float(v[DEX_LIQUIDITY_METRIC_MAP[v['protocol']]]) < MAX_LIQUIDITY_THRESHOLD}\n",
    "\n",
    "    trimmed_sorted_pools = list(filter(lambda pool: float(pool[DEX_LIQUIDITY_METRIC_MAP[pool['protocol']]]) > MIN_LIQUIDITY_THRESHOLD and float(pool[DEX_LIQUIDITY_METRIC_MAP[pool['protocol']]]) < MAX_LIQUIDITY_THRESHOLD, trimmed_sorted_pools))\n",
    "\n",
    "    # Create a dictionary to store prices and identify tokens\n",
    "    price_dict = {} # Dictionary for storing prices\n",
    "    exchange_dict = {} # Dictionary for storing exchanges\n",
    "    movement_dict_5m = {}  # Dictionary for storing price movement percentages for 5 minutes\n",
    "    movement_dict_1h = {}  # Dictionary for storing price movement percentages for 1 hour\n",
    "    movement_dict_24h = {}  # Dictionary for storing price movement percentages for 24 hours\n",
    "    token_set = set() # Used to avoid duplicates\n",
    "    TOKEN_SYMBOL_MAP = {} # Dictionary for storing token ids:symbols\n",
    "    TOKEN_NAME_MAP = {} # Dictionary for storing token ids:names\n",
    "\n",
    "    # Populate the dictionaries\n",
    "    for pool in trimmed_sorted_pools:\n",
    "        token0_id = pool['token0']['id']\n",
    "        token1_id = pool['token1']['id']\n",
    "        token0_symbol = pool['token0']['symbol']\n",
    "        token1_symbol = pool['token1']['symbol']\n",
    "        TOKEN_SYMBOL_MAP[token0_id] = token0_symbol\n",
    "        TOKEN_SYMBOL_MAP[token1_id] = token1_symbol\n",
    "        TOKEN_NAME_MAP[token0_id] = pool['token0']['name']\n",
    "        TOKEN_NAME_MAP[token1_id] = pool['token1']['name']\n",
    "        token_set.add(token0_id)\n",
    "        token_set.add(token1_id)\n",
    "        try:\n",
    "            price0 = float(pool['token0'].get('priceUSD', 0))\n",
    "            price1 = float(pool['token1'].get('priceUSD', 0))\n",
    "            timestamp = time.time()  # Current time in seconds since the Epoch\n",
    "            if price0 != 0 and price1 != 0:\n",
    "                pair1 = (token0_id, token1_id)\n",
    "                pair2 = (token1_id, token0_id)\n",
    "                if pair1 not in price_dict:\n",
    "                    price_dict[pair1] = []\n",
    "                if pair2 not in price_dict:\n",
    "                    price_dict[pair2] = []\n",
    "                price_dict[pair1].append((timestamp, price1))\n",
    "                price_dict[pair2].append((timestamp, price0))\n",
    "                if pair1 not in exchange_dict:\n",
    "                    exchange_dict[pair1] = []\n",
    "                if pair2 not in exchange_dict:\n",
    "                    exchange_dict[pair2] = []\n",
    "                exchange_dict[pair1].append(pool['protocol'])\n",
    "                exchange_dict[pair2].append(pool['protocol'])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    movement_df_5m = pd.DataFrame(0, index=token_set, columns=token_set)  # DataFrame for storing price movement percentages\n",
    "    movement_df_1h = pd.DataFrame(0, index=token_set, columns=token_set)  # DataFrame for storing 1-hour price movement percentages\n",
    "    movement_df_24h = pd.DataFrame(0, index=token_set, columns=token_set)  # DataFrame for storing 24-hour price movement percentages\n",
    "\n",
    "    # Compute the price movement percentage\n",
    "    for pair, prices in price_dict.items():\n",
    "        current_price = prices[-1][1]\n",
    "        past_price_5m = None\n",
    "        past_price_1h = None\n",
    "        past_price_24h = None\n",
    "\n",
    "        for timestamp, price in reversed(prices):\n",
    "            if past_price_5m is None and timestamp <= time.time() - 60*5:  # Find the price 5 minutes ago\n",
    "                past_price_5m = price\n",
    "            if past_price_1h is None and timestamp <= time.time() - 60*60:  # Find the price 1 hour ago\n",
    "                past_price_1h = price\n",
    "            if past_price_24h is None and timestamp <= time.time() - 24*60*60:  # Find the price 24 hours ago\n",
    "                past_price_24h = price\n",
    "            if past_price_5m is not None and past_price_1h is not None and past_price_24h is not None:\n",
    "                break\n",
    "\n",
    "        movement_dict_5m[pair] = ((current_price - (past_price_5m or current_price)) / (past_price_5m or current_price)) * 100\n",
    "        movement_dict_1h[pair] = ((current_price - (past_price_1h or current_price)) / (past_price_1h or current_price)) * 100\n",
    "        movement_dict_24h[pair] = ((current_price - (past_price_24h or current_price)) / (past_price_24h or current_price)) * 100\n",
    "\n",
    "\n",
    "    # Create an empty DataFrame for storing liquidity data\n",
    "    liquidity_df = pd.DataFrame(0, index=token_set, columns=token_set) # DataFrame for storing liquidity\n",
    "    # Create an empty DataFrame for storing volume data\n",
    "    volume_df_24h = pd.DataFrame(0, index=token_set, columns=token_set)\n",
    "    volume_df_1h = pd.DataFrame(0, index=token_set, columns=token_set)\n",
    "    # Dict for storing which pools pairs can be found in\n",
    "    pool_dict_for_pairs = {}\n",
    "\n",
    "    # Populate the DataFrames\n",
    "    for pool in trimmed_sorted_pools:\n",
    "        token0_id = pool['token0']['id']\n",
    "        token1_id = pool['token1']['id']\n",
    "\n",
    "        if token0_id in TOKEN_SYMBOL_MAP and token1_id in TOKEN_SYMBOL_MAP:\n",
    "            try:\n",
    "                # Compute liquidity\n",
    "                liquidity = float(pool[DEX_LIQUIDITY_METRIC_MAP[pool['protocol']]])\n",
    "                liquidity_df.loc[token0_id, token1_id] += liquidity\n",
    "                liquidity_df.loc[token1_id, token0_id] += liquidity\n",
    "\n",
    "                # Compute volume\n",
    "                volume_24h = float(pool['volume_24h'])\n",
    "                volume_1h = float(pool['volume_1h'])\n",
    "                volume_df_24h.loc[token0_id, token1_id] += volume_24h\n",
    "                volume_df_24h.loc[token1_id, token0_id] += volume_24h\n",
    "                volume_df_1h.loc[token0_id, token1_id] += volume_1h\n",
    "                volume_df_1h.loc[token1_id, token0_id] += volume_1h\n",
    "                \n",
    "                # Compute price ratio\n",
    "                price0 = float(pool['token0'].get('priceUSD', 0))\n",
    "                price1 = float(pool['token1'].get('priceUSD', 0))\n",
    "\n",
    "                if price0 != 0 and price1 != 0:\n",
    "                    pair1 = (token0_id, token1_id)\n",
    "                    pair2 = (token1_id, token0_id)\n",
    "\t\n",
    "                    if pair1 not in pool_dict_for_pairs:\n",
    "                        pool_dict_for_pairs[pair1] = []\n",
    "                    if pair2 not in pool_dict_for_pairs:\n",
    "                        pool_dict_for_pairs[pair2] = []\n",
    "                    pool_dict_for_pairs[pair1].append(pool['id'])\n",
    "                    pool_dict_for_pairs[pair2].append(pool['id'])\n",
    "\n",
    "                    movement_df_5m.loc[pair1] = movement_dict_5m.get(pair1, 0)\n",
    "                    movement_df_5m.loc[pair2] = movement_dict_5m.get(pair2, 0)\n",
    "                    movement_df_1h.loc[pair1] = movement_dict_1h.get(pair1, 0)\n",
    "                    movement_df_1h.loc[pair2] = movement_dict_1h.get(pair2, 0)\n",
    "                    movement_df_24h.loc[pair1] = movement_dict_24h.get(pair1, 0)\n",
    "                    movement_df_24h.loc[pair2] = movement_dict_24h.get(pair2, 0)\n",
    "\n",
    "            except Exception as e:\n",
    "                # print(f\"Error with token pair ({TOKEN_SYMBOL_MAP[token0_id]}, {TOKEN_SYMBOL_MAP[token1_id]}): {e}\")\n",
    "                continue\n",
    "\n",
    "    # Convert tuple keys to string\n",
    "    str_price_dict = {str(k): v for k, v in price_dict.items()}\n",
    "\n",
    "    # Save price_dict to data\n",
    "    with open('data/price_dict.json', 'w') as f:\n",
    "        json.dump(str_price_dict, f)\n",
    "\n",
    "    # Create a DataFrame for average prices\n",
    "    average_price_df = pd.DataFrame(np.nan, index=token_set, columns=token_set)\n",
    "    movement_df_5m.replace(0, np.nan, inplace=True)  # Replace 0s with NaN for calculating percentage movements\n",
    "    movement_df_1h.replace(0, np.nan, inplace=True)  # Replace 0s with NaN for calculating percentage movements\n",
    "    movement_df_24h.replace(0, np.nan, inplace=True)  # Replace 0s with NaN for calculating percentage movements\n",
    "\n",
    "    # Compute IQR for outlier detection for each pair\n",
    "    trimmed_average_price_dict = {}\n",
    "    limit = 1.5\n",
    "\n",
    "    for k, v in price_dict.items():\n",
    "        if v: # if there are prices\n",
    "            Q1 = np.quantile(v, 0.25)\n",
    "            Q3 = np.quantile(v, 0.75)\n",
    "            IQR = Q3 - Q1\n",
    "\n",
    "            # Define bounds for outliers\n",
    "            lower_bound = Q1 - limit * IQR\n",
    "            upper_bound = Q3 + limit * IQR\n",
    "\n",
    "            # Trim outliers\n",
    "            trimmed_prices = [price[1] for price in v if lower_bound <= price[1] <= upper_bound]\n",
    "            # check if there are any trimmed prices using a.any()\n",
    "            if trimmed_prices: # avoid division by zero\n",
    "                trimmed_average_price_dict[k] = sum(trimmed_prices) / len(trimmed_prices)\n",
    "\n",
    "    # Convert tuple keys to string\n",
    "    str_trimmed_average_price_dict = {str(k): v for k, v in trimmed_average_price_dict.items()}\n",
    "\n",
    "    # Populate the average_price_df using the trimmed_average_price_dict\n",
    "    for (token0, token1), average_price in trimmed_average_price_dict.items():\n",
    "        average_price_df.loc[token0, token1] = average_price\n",
    "\n",
    "    # Save the trimmed average price dictionary to a file with a timestamp\n",
    "    unix_timestamp_str = str(int(time.time()))\n",
    "    trimmed_average_price_dict_path = f'data/trimmed_average_price_dict_{unix_timestamp_str}.json'\n",
    "    with open(trimmed_average_price_dict_path, 'w') as f:\n",
    "        json.dump(str_trimmed_average_price_dict, f)\n",
    "\n",
    "    # Find the file that was created closest to 5 minutes ago\n",
    "    filepaths = glob.glob('data/trimmed_average_price_dict_*.json')\n",
    "\n",
    "    # Compute the price movement percentage using the current prices and the prices from 5 minutes, 1 hour and 24 hours ago\n",
    "    closest_filepath_5m, closest_filepath_1h, closest_filepath_24h = None, None, None\n",
    "    closest_diff_5m, closest_diff_1h, closest_diff_24h = None, None, None\n",
    "    for filepath in filepaths:\n",
    "        file_timestamp_str = filepath.split('_')[-1].split('.')[0]\n",
    "        file_timestamp = int(file_timestamp_str)\n",
    "        diff_5m = abs(time.time() - file_timestamp - 60*5)\n",
    "        diff_1h = abs(time.time() - file_timestamp - 60*60)\n",
    "        diff_24h = abs(time.time() - file_timestamp - 60*60*24)\n",
    "\n",
    "        # Delete any files older than 25 hours\n",
    "        if diff_24h > 60*60*25:\n",
    "            os.remove(filepath)\n",
    "\n",
    "        if closest_diff_5m is None or diff_5m < closest_diff_5m:\n",
    "            closest_diff_5m = diff_5m\n",
    "            closest_filepath_5m = filepath\n",
    "        if closest_diff_1h is None or diff_1h < closest_diff_1h:\n",
    "            closest_diff_1h = diff_1h\n",
    "            closest_filepath_1h = filepath\n",
    "        if closest_diff_24h is None or diff_24h < closest_diff_24h:\n",
    "            closest_diff_24h = diff_24h\n",
    "            closest_filepath_24h = filepath\n",
    "\n",
    "    past_price_dict_5m = {}\n",
    "    if closest_filepath_5m:\n",
    "        with open(closest_filepath_5m, 'r') as f:\n",
    "            past_price_dict_5m = json.load(f)\n",
    "    past_price_dict_1h = {}\n",
    "    if closest_filepath_1h:\n",
    "        with open(closest_filepath_1h, 'r') as f:\n",
    "            past_price_dict_1h = json.load(f)\n",
    "    past_price_dict_24h = {}\n",
    "    if closest_filepath_24h:\n",
    "        with open(closest_filepath_24h, 'r') as f:\n",
    "            past_price_dict_24h = json.load(f)\n",
    "\n",
    "    for (token0, token1), current_price in trimmed_average_price_dict.items():\n",
    "        past_price_5m = past_price_dict_5m.get(str((token0, token1)), current_price)\n",
    "        price_movement_5m = ((current_price - past_price_5m) / past_price_5m) * 100\n",
    "        movement_df_5m.loc[token0, token1] = price_movement_5m\n",
    "        past_price_1h = past_price_dict_1h.get(str((token0, token1)), current_price)\n",
    "        price_movement_1h = ((current_price - past_price_1h) / past_price_1h) * 100\n",
    "        movement_df_1h.loc[token0, token1] = price_movement_1h\n",
    "        past_price_24h = past_price_dict_24h.get(str((token0, token1)), current_price)\n",
    "        price_movement_24h = ((current_price - past_price_24h) / past_price_24h) * 100\n",
    "        movement_df_24h.loc[token0, token1] = price_movement_24h\n",
    "\n",
    "    # Compute total liquidity for each token\n",
    "    all_ids = list(set(liquidity_df.columns).union(set(liquidity_df.index)))\n",
    "    row_sums = liquidity_df.sum(axis=0).reindex(all_ids, fill_value=0)\n",
    "    col_sums = liquidity_df.sum(axis=1).reindex(all_ids, fill_value=0)\n",
    "    liquidity_totals = row_sums + col_sums\n",
    "\n",
    "    # Sort by total liquidity\n",
    "    sorted_ids = liquidity_totals.sort_values(ascending=False).index\n",
    "\n",
    "    # Reindex DataFrames according to sorted liquidity\n",
    "    liquidity_df = liquidity_df.reindex(index=sorted_ids, columns=sorted_ids)\n",
    "    average_price_df = average_price_df.reindex(index=sorted_ids, columns=sorted_ids)\n",
    "    movement_df_5m = movement_df_5m.reindex(index=sorted_ids, columns=sorted_ids)\n",
    "    movement_df_1h = movement_df_1h.reindex(index=sorted_ids, columns=sorted_ids)\n",
    "    movement_df_24h = movement_df_24h.reindex(index=sorted_ids, columns=sorted_ids)\n",
    "\n",
    "    # Remove the diagonal values as they don't represent valid token pairs\n",
    "    np.fill_diagonal(liquidity_df.values, np.nan)\n",
    "    np.fill_diagonal(average_price_df.values, np.nan)\n",
    "    np.fill_diagonal(movement_df_5m.values, np.nan)\n",
    "    np.fill_diagonal(movement_df_1h.values, np.nan)\n",
    "    np.fill_diagonal(movement_df_24h.values, np.nan)\n",
    "\n",
    "    # replace all nans with None\n",
    "    liquidity_df = liquidity_df.where(pd.notnull(liquidity_df), None)\n",
    "    average_price_df = average_price_df.where(pd.notnull(average_price_df), None)\n",
    "    movement_df_5m = movement_df_5m.where(pd.notnull(movement_df_5m), None)\n",
    "    movement_df_1h = movement_df_1h.where(pd.notnull(movement_df_1h), None)\n",
    "    movement_df_24h = movement_df_24h.where(pd.notnull(movement_df_24h), None)\n",
    "\n",
    "    # Create a combined DataFrame\n",
    "    combined_df = pd.DataFrame(index=liquidity_df.index, columns=liquidity_df.columns)\n",
    "\n",
    "    for row_id in combined_df.index:\n",
    "        for col_id in combined_df.columns:\n",
    "            liquidity = liquidity_df.loc[row_id, col_id] if pd.notnull(liquidity_df.loc[row_id, col_id]) else 0\n",
    "            avg_price = average_price_df.loc[row_id, col_id] if pd.notnull(average_price_df.loc[row_id, col_id]) else 0\n",
    "            price_movement_5m = movement_df_5m.loc[row_id, col_id] if pd.notnull(movement_df_5m.loc[row_id, col_id]) else 0\n",
    "            price_movement_1h = movement_df_1h.loc[row_id, col_id] if pd.notnull(movement_df_1h.loc[row_id, col_id]) else 0\n",
    "            price_movement_24h = movement_df_24h.loc[row_id, col_id] if pd.notnull(movement_df_24h.loc[row_id, col_id]) else 0\n",
    "            volume_24h = volume_df_24h.loc[row_id, col_id]\n",
    "            volume_1h = volume_df_1h.loc[row_id, col_id]\n",
    "            safety_score = random.randrange(0, 5)\n",
    "            pair = {}\n",
    "            pair[row_id] = {\n",
    "                'id': row_id,  \n",
    "                'symbol': TOKEN_SYMBOL_MAP[row_id],\n",
    "                'name': TOKEN_NAME_MAP[row_id],\n",
    "            }\n",
    "            pair[col_id] = {\n",
    "                'id': col_id,  \n",
    "                'symbol': TOKEN_SYMBOL_MAP[col_id],\n",
    "                'name': TOKEN_NAME_MAP[col_id],\n",
    "            }\n",
    "\n",
    "            combined_df.at[row_id, col_id] = {\n",
    "                'pair': pair,\n",
    "                'liquidity': liquidity,\n",
    "                'average_price': avg_price,\n",
    "                'price_movement_5m': price_movement_5m,\n",
    "                'price_movement_1h': price_movement_1h,\n",
    "                'price_movement_24h': price_movement_24h,\n",
    "                'volume_1h': volume_1h,\n",
    "                'volume_24h': volume_24h,\n",
    "                'safety_score': safety_score,\n",
    "                'exchanges': set(exchange_dict.get((row_id, col_id), [])),\n",
    "                'pools': list(set(pool_dict_for_pairs.get((row_id, col_id), [])))\n",
    "            }\n",
    "            # check if the cell is a diagonal\n",
    "            if row_id == col_id:\n",
    "                combined_df.at[row_id, col_id]['diagonal'] = True\n",
    "                # set all values to 0\n",
    "                combined_df.at[row_id, col_id]['liquidity'] = 0\n",
    "                combined_df.at[row_id, col_id]['average_price'] = 0\n",
    "                combined_df.at[row_id, col_id]['price_movement_5m'] = 0\n",
    "                combined_df.at[row_id, col_id]['price_movement_1h'] = 0\n",
    "                combined_df.at[row_id, col_id]['price_movement_24h'] = 0\n",
    "                combined_df.at[row_id, col_id]['volume_24h'] = 0\n",
    "                combined_df.at[row_id, col_id]['safety_score'] = 0\n",
    "                combined_df.at[row_id, col_id]['exchanges'] = []\n",
    "                combined_df.at[row_id, col_id]['pools'] = []\n",
    "            else:\n",
    "                combined_df.at[row_id, col_id]['diagonal'] = False\n",
    "            # if the pool is not found in any pools or exchanges then any nonzero data is invalid\n",
    "            if combined_df.at[row_id, col_id]['pools'] == [] or combined_df.at[row_id, col_id]['exchanges'] == []:\n",
    "                combined_df.at[row_id, col_id]['liquidity'] = 0\n",
    "                combined_df.at[row_id, col_id]['average_price'] = 0\n",
    "                combined_df.at[row_id, col_id]['price_movement_5m'] = 0\n",
    "                combined_df.at[row_id, col_id]['price_movement_1h'] = 0\n",
    "                combined_df.at[row_id, col_id]['price_movement_24h'] = 0\n",
    "                combined_df.at[row_id, col_id]['volume_24h'] = 0\n",
    "                combined_df.at[row_id, col_id]['safety_score'] = 0\n",
    "    # combined_df.to_csv('data/combined_df.csv')\n",
    "    combined_df.to_json('data/combined_df_liquidity.json', orient='split')\n",
    "\n",
    "    combined_df['mean_avg_price'] = combined_df.applymap(lambda x: x['average_price']).mean(axis=1)\n",
    "    combined_df.sort_values(by=['mean_avg_price'], ascending=False, inplace=True)\n",
    "    del combined_df['mean_avg_price']\n",
    "    combined_df = combined_df.reindex(combined_df.index, axis=1)\n",
    "    combined_df.to_json('data/combined_df_average_price.json', orient='split')\n",
    "\n",
    "    combined_df['max_volume_24h'] = combined_df.applymap(lambda x: x['volume_24h']).max(axis=1)\n",
    "    combined_df.sort_values(by=['max_volume_24h'], ascending=False, inplace=True)\n",
    "    del combined_df['max_volume_24h']\n",
    "    combined_df = combined_df.reindex(combined_df.index, axis=1)\n",
    "    combined_df.to_json('data/combined_df_volume_24h.json', orient='split')\n",
    "\n",
    "    # Move USDT, USDC, and DAI to the top 3 rows\n",
    "    combined_df = combined_df.reindex(['0xdac17f958d2ee523a2206206994597c13d831ec7', '0xa0b86991c6218b36c1d19d4a2e9eb0ce3606eb48', '0x6b175474e89094c44da98b954eedeac495271d0f'] + list(combined_df.index.difference(['0xdac17f958d2ee523a2206206994597c13d831ec7', '0xa0b86991c6218b36c1d19d4a2e9eb0ce3606eb48', '0x6b175474e89094c44da98b954eedeac495271d0f'])), axis=0)\n",
    "\n",
    "    # Make the columns have the same order as the rows\n",
    "    combined_df = combined_df.reindex(combined_df.index, axis=1)\n",
    "\n",
    "    combined_df.to_json('data/combined_df_popular.json', orient='split')\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = refresh_matrix(pool_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pair': {'0xa0b86991c6218b36c1d19d4a2e9eb0ce3606eb48': {'id': '0xa0b86991c6218b36c1d19d4a2e9eb0ce3606eb48',\n",
       "   'symbol': 'USDC',\n",
       "   'name': 'USD Coin'},\n",
       "  '0xdac17f958d2ee523a2206206994597c13d831ec7': {'id': '0xdac17f958d2ee523a2206206994597c13d831ec7',\n",
       "   'symbol': 'USDT',\n",
       "   'name': 'Tether USD'}},\n",
       " 'liquidity': 564318495.5915989,\n",
       " 'average_price': 1.0000892231676672,\n",
       " 'price_movement_5m': 0.0,\n",
       " 'price_movement_1h': 0.0,\n",
       " 'price_movement_24h': 0.0,\n",
       " 'volume_1h': 232114.66257084618,\n",
       " 'volume_24h': 53428333.701350346,\n",
       " 'safety_score': 3,\n",
       " 'exchanges': {'Balancer_V2', 'Curve'},\n",
       " 'pools': ['0xbd3a698826d27563d08d459faff2d5f6960e21cf',\n",
       "  '0x6aebf3f0829347b310d16b4c03a1d3b3838496aa',\n",
       "  '0xf4e806b0fd2b278bd45b3bffe66c2c77bcf700be',\n",
       "  '0xc3141fc45791cca3f21f2a926fd8598c39a4c6d2',\n",
       "  '0x664e158dc6d1c0aacec01c59a9d11643f1a97e8a',\n",
       "  '0x9f383f91c89cbd649c700c2bf69c2a828af299aa',\n",
       "  '0x81b7f92c7b7d9349b989b4982588761bfa1aa627',\n",
       "  '0x09a4551eea71d368358fa42acc9bed8d51566f2e',\n",
       "  '0x0d70f2fc0096b9e3d39ad8a9c011126fbdb69779',\n",
       "  '0xbd482ffb3e6e50dc1c437557c3bea2b68f3683ee',\n",
       "  '0x06df3b2bbb68adc8b0e302443692037ed9f91b42',\n",
       "  '0x37306aec67220b11991416e1e7a97f9caeeaff7d',\n",
       "  '0xbebc44782c7db0a1a60cb6fe97d0b483032ff1c7',\n",
       "  '0x1d1b44a4c5a047bdc9b3f06842566e5ea9c1966d',\n",
       "  '0x79c58f70905f734641735bc61e45c19dd9ad60bc',\n",
       "  '0xa3353f841193c4b77080038663a0db22da7d26db',\n",
       "  '0xa5407eae9ba41422680e2e00537571bcc53efbfd',\n",
       "  '0x9691350ce007cc2c3bf8819ca2bc9965118f4495',\n",
       "  '0x7e1e4ed284d34722a853cc002f4b990f139dad32'],\n",
       " 'diagonal': False}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.iloc[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pair': {'0xa0b86991c6218b36c1d19d4a2e9eb0ce3606eb48': {'id': '0xa0b86991c6218b36c1d19d4a2e9eb0ce3606eb48',\n",
       "   'symbol': 'USDC',\n",
       "   'name': 'USD Coin'},\n",
       "  '0xdac17f958d2ee523a2206206994597c13d831ec7': {'id': '0xdac17f958d2ee523a2206206994597c13d831ec7',\n",
       "   'symbol': 'USDT',\n",
       "   'name': 'Tether USD'}},\n",
       " 'liquidity': 564318495.5915989,\n",
       " 'average_price': 1.0000892231676672,\n",
       " 'price_movement_5m': 3.241329714160821,\n",
       " 'price_movement_1h': -4.9102702735792825,\n",
       " 'price_movement_24h': -31.925818263578044,\n",
       " 'volume_1h': 232114.66257084618,\n",
       " 'volume_24h': 53428333.701350346,\n",
       " 'safety_score': 2,\n",
       " 'exchanges': {'Balancer_V2', 'Curve'},\n",
       " 'pools': ['0xbd3a698826d27563d08d459faff2d5f6960e21cf',\n",
       "  '0x6aebf3f0829347b310d16b4c03a1d3b3838496aa',\n",
       "  '0xf4e806b0fd2b278bd45b3bffe66c2c77bcf700be',\n",
       "  '0xc3141fc45791cca3f21f2a926fd8598c39a4c6d2',\n",
       "  '0x664e158dc6d1c0aacec01c59a9d11643f1a97e8a',\n",
       "  '0x9f383f91c89cbd649c700c2bf69c2a828af299aa',\n",
       "  '0x81b7f92c7b7d9349b989b4982588761bfa1aa627',\n",
       "  '0x09a4551eea71d368358fa42acc9bed8d51566f2e',\n",
       "  '0x0d70f2fc0096b9e3d39ad8a9c011126fbdb69779',\n",
       "  '0xbd482ffb3e6e50dc1c437557c3bea2b68f3683ee',\n",
       "  '0x06df3b2bbb68adc8b0e302443692037ed9f91b42',\n",
       "  '0x37306aec67220b11991416e1e7a97f9caeeaff7d',\n",
       "  '0xbebc44782c7db0a1a60cb6fe97d0b483032ff1c7',\n",
       "  '0x1d1b44a4c5a047bdc9b3f06842566e5ea9c1966d',\n",
       "  '0x79c58f70905f734641735bc61e45c19dd9ad60bc',\n",
       "  '0xa3353f841193c4b77080038663a0db22da7d26db',\n",
       "  '0xa5407eae9ba41422680e2e00537571bcc53efbfd',\n",
       "  '0x9691350ce007cc2c3bf8819ca2bc9965118f4495',\n",
       "  '0x7e1e4ed284d34722a853cc002f4b990f139dad32'],\n",
       " 'diagonal': False}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this cell assumes you have a file called 'trimmed_average_price_dict.json' in the data folder \n",
    "# which can be generated by running the above cells then removing the timestamp\n",
    "\n",
    "def randomize_prices(price_dict, lower, upper):\n",
    "    for key, price in price_dict.items():\n",
    "        random_change = random.uniform(lower, upper)  # Simulate up to +/-5% change\n",
    "        price_dict[key] += price * random_change\n",
    "        # price_dict[key] = max(price_dict[key], 0)  # Prevent negative prices\n",
    "    return price_dict\n",
    "\n",
    "# delete existing files begining with 'trimmed_average_price_dict_'\n",
    "for filename in os.listdir('data'):\n",
    "    if filename.startswith('trimmed_average_price_dict_'):\n",
    "        os.remove(f'data/{filename}')\n",
    "\n",
    "with open('data/trimmed_average_price_dict.json', 'r') as f:\n",
    "    price_dict = json.load(f)\n",
    "# timestamp 5 minutes ago\n",
    "time_5m_ago = str(int(time.time()) - 300)\n",
    "# timestamp 1 hour ago\n",
    "time_1h_ago = str(int(time.time()) - 3600)\n",
    "# timestamp 24 hours ago\n",
    "time_24h_ago = str(int(time.time()) - 86400)\n",
    "# randomize then save with timestamp 5 minutes ago\n",
    "price_dict = randomize_prices(price_dict, -0.05, 0)\n",
    "with open(f'data/trimmed_average_price_dict_{time_5m_ago}.json', 'w') as f:\n",
    "    json.dump(price_dict, f)\n",
    "# randomize then save with timestamp 1 hour ago\n",
    "price_dict = randomize_prices(price_dict, 0.05, 0.1)\n",
    "with open(f'data/trimmed_average_price_dict_{time_1h_ago}.json', 'w') as f:\n",
    "    json.dump(price_dict, f)\n",
    "# randomize then save with timestamp 24 hours ago\n",
    "price_dict = randomize_prices(price_dict, 0.1, 0.5)\n",
    "with open(f'data/trimmed_average_price_dict_{time_24h_ago}.json', 'w') as f:\n",
    "    json.dump(price_dict, f)\n",
    "    \n",
    "df = refresh_matrix(pool_dict)\n",
    "df.iloc[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find any pools containing the token 0xBB0E17EF65F82Ab018d8EDd776e8DD940327B28b, check token0 id and token1 id\n",
    "pancake_pools = {}\n",
    "for pool in pool_dict:\n",
    "    if pool_dict[pool]['protocol'] == PANCAKESWAP_V3:\n",
    "        pancake_pools[pool] = pool_dict[pool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0x2201d2400d30bfd8172104b4ad046d019ca4e7bd\n"
     ]
    }
   ],
   "source": [
    "# find this pool in pancake_pools (pool['id']) 0x14bf727f67aa294ec36347bd95aba1a2c136fe7a\n",
    "for pool in pancake_pools:\n",
    "    if pancake_pools[pool]['token0']['symbol'] == 'rETH':\n",
    "        print(pancake_pools[pool]['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token0': {'id': '0xae78736cd615f374d3085123a210448e74fc6393',\n",
       "  'symbol': 'rETH',\n",
       "  'name': 'Rocket Pool ETH',\n",
       "  'decimals': '18',\n",
       "  'priceUSD': 9650.03247550595},\n",
       " 'token1': {'symbol': 'WETH',\n",
       "  'name': 'Wrapped Ether',\n",
       "  'id': '0xc02aaa39b223fe8d0a0e5c4f27ead9083c756cc2',\n",
       "  'decimals': '18',\n",
       "  'priceUSD': 3756.3932191782924},\n",
       " 'id': '0x2201d2400d30bfd8172104b4ad046d019ca4e7bd',\n",
       " 'totalValueLockedUSD': '11333901.07073927639756095699101033',\n",
       " 'liquidity': '7664164044945939780261294',\n",
       " 'token0Price': '0.9289361843887342559924706423400394',\n",
       " 'token1Price': '1.076500212614742425377922025008424',\n",
       " 'feeTier': '500',\n",
       " 'sqrtPrice': '82202806234021784419778492760',\n",
       " 'volumeUSD': '15130240.91741341145297878051023574',\n",
       " 'poolHourData': [{'volumeUSD': '1162320.477473803226683286061690536'}],\n",
       " 'poolDayData': [{'volumeUSD': '1444678.028438683414977492298177048'}],\n",
       " 'protocol': 'PancakeSwap_V3',\n",
       " 'reserve0': '1174.493567716728451839',\n",
       " 'reserve1': '3017.229669373793185258',\n",
       " 'volume_24h': 0,\n",
       " 'volume_1h': 0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for pool in pancake_pools:\n",
    "    if pancake_pools[pool]['id'] == '0x2201d2400d30bfd8172104b4ad046d019ca4e7bd':\n",
    "        a = pancake_pools[pool]\n",
    "        \n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
